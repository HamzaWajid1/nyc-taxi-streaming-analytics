
# ğŸ“Š NYC Taxi Real-Time Streaming & Analytics Pipeline

This project builds a complete **end-to-end real-time data engineering pipeline** using the NYC Yellow Taxi dataset.
The system is developed **locally first** (Kafka + PySpark + Postgres + Power BI), then fully deployed on **AWS (Kinesis + S3 + Glue + RDS)**.

The goal is to simulate real-time taxi rides, process them using a modern data lake architecture (Bronze â†’ Silver â†’ Gold), and build analytics dashboards to extract meaningful insights.

---

## ğŸš€ Project Architecture (High-Level)

### **Local Pipeline (Phases 1â€“4)**

* **Python Data Generator** â†’ simulates real-time taxi events
* **Kafka** â†’ streaming event broker
* **PySpark Structured Streaming** â†’ ingestion + raw â†’ Bronze
* **PySpark ETL** â†’ Bronze â†’ Silver â†’ Gold transformation
* **PostgreSQL** â†’ analytics warehouse
* **Power BI** â†’ visual analytics / dashboards

### **AWS Cloud Pipeline (Phase 5+)**

* **Kinesis Data Streams** â†’ real-time ingestion
* **AWS Glue / PySpark** â†’ ETL
* **S3** â†’ Bronze/Silver/Gold data lake
* **AWS RDS (Postgres)** â†’ analytics layer
* **Power BI / QuickSight** â†’ dashboards
* **Step Functions / Airflow** â†’ orchestration
* **CloudWatch** â†’ monitoring

---

## ğŸ“ Repository Structure

```
nyc-taxi-streaming-analytics/
â”‚
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .gitignore
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/        # original CSV files (not pushed to GitHub)
â”‚   â”œâ”€â”€ samples/    # small subsets for quick experimentation
â”‚
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ exploration.ipynb   # dataset understanding (Phase 0)
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ generator/          # real-time data generator (Phase 1)
â”‚   â”œâ”€â”€ streaming/          # Kafka + Spark jobs (Phase 1)
â”‚   â”œâ”€â”€ etl/                # Bronze â†’ Silver â†’ Gold ETL (Phase 2)
â”‚   â”œâ”€â”€ warehouse/          # Postgres loaders (Phase 3)
â”‚   â”œâ”€â”€ dashboard/          # Power BI queries, notes (Phase 4)
â”‚
â””â”€â”€ docker/
    â””â”€â”€ kafka-compose.yml   # Kafka + Zookeeper setup (Phase 1)
```

---

## ğŸŸ¦ Project Phases

### **ğŸ”µ Phase 0 â€” Environment Setup & Dataset Exploration (Current Phase)**

* Install Python, Kafka, PySpark, PostgreSQL, Power BI
* Set up repo structure
* Download NYC Taxi raw dataset
* Create a sample subset for testing
* Build initial exploration notebook
* Understand schema, missing values, data ranges

âœ” *This is the phase we are working on right now.*
âœ” *No pipeline logic is written yet.*

---

### **ğŸŸ¢ Phase 1 â€” Real-Time Data Simulation (Local Kafka)**

* Python script generates taxi ride events
* Events published to Kafka topic: `taxi-events`
* PySpark Structured Streaming consumes Kafka events
* Raw JSON written to Bronze layer (local folder)

---

### **ğŸŸ¡ Phase 2 â€” ETL Pipeline (Bronze â†’ Silver â†’ Gold)**

* Data cleaning, validation, and enrichment
* Feature engineering: trip duration, borough mapping, tip %
* Aggregations for analytics dashboards

---

### **ğŸŸ£ Phase 3 â€” Warehouse Loading (Postgres)**

* Load Gold-layer aggregates into warehouse
* SQL queries for KPIs
* Power BI-ready tables

---

### **ğŸŸ  Phase 4 â€” Analytics Dashboard (Power BI)**

* Peak hours, busiest zones
* Revenue metrics & fare analysis
* Tip behaviour patterns
* Route/zone heatmaps

---

### **ğŸ”´ Phase 5 â€” AWS Deployment**

* Replace Kafka â†’ Kinesis
* Replace local PySpark â†’ AWS Glue
* Store Bronze/Silver/Gold in S3
* Load aggregates into RDS
* Power BI connects to AWS

---

## ğŸ¯ Final Deliverables

* Full **real-time** & **batch** data pipeline
* Proper **Bronze/Silver/Gold** architecture
* Complete **AWS migration**
* Professional **Power BI dashboard**
* Multiple **Medium articles** documenting each phase
* A strong **data engineering portfolio project**

---

## ğŸ“Œ Current Status: *Phase 0*

Environment setup in progress:
âœ” Repository created
âœ” Folder structure created
âœ” Requirements and gitignore ready
ğŸš§ Dataset download and exploration notebook next

---

## ğŸ“¬ Contact / Notes

This project is designed for portfolio-building and interview preparation for **Data Engineering** roles.
